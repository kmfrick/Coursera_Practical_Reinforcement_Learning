{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOlFdCK_xWUV"
   },
   "source": [
    "## Q-learning\n",
    "\n",
    "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
    "\n",
    "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30639,
     "status": "ok",
     "timestamp": 1632654247409,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "fO3MuW4SxWUa",
    "outputId": "13c25f63-3efd-430b-df0c-6a5b6c571861"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week3_model_free/submit.py\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1632654257111,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "YUSBxCIixWUd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1632664917230,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "I6WITxHHxWUe"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly. \n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "        qvals = []\n",
    "        for a in possible_actions:\n",
    "          qvals.append(self.get_qvalue(state, a))\n",
    "        return max(qvals)\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        oldq = self.get_qvalue(state, action)\n",
    "        newq = (1 - learning_rate) * oldq + learning_rate * (reward + gamma * self.get_value(next_state))\n",
    "        self.set_qvalue(state, action, newq)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        idx = []\n",
    "        qvals = []\n",
    "        for a in possible_actions:\n",
    "          qvals.append(self.get_qvalue(state, a))\n",
    "          idx.append(a)\n",
    "\n",
    "        return idx[np.argmax(qvals)]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list). \n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        p = np.random.uniform(0, 1)\n",
    "        if p < epsilon:\n",
    "          return np.random.choice(possible_actions)\n",
    "        else:\n",
    "          return self.get_best_action(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElK3jZTExWUg"
   },
   "source": [
    "### Try it on taxi\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1632664919883,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "YxFvn8GAxWUh"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1632664920951,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "k40RUXbLxWUh"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1632664790194,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "wxFMStOxxWUh"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "\n",
    "        # train (update) agent for state s\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 3867,
     "status": "ok",
     "timestamp": 1632664928056,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "RVU9GnNVxWUi",
    "outputId": "5e39f43a-19ec-4579-8ff2-7448a34afe95"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbxnC8woxWUk"
   },
   "source": [
    "### Submit to Coursera I: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1632664930328,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "nqVkDKjjxWUk"
   },
   "outputs": [],
   "source": [
    "submit_rewards1 = rewards.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdCW2NfXxWUl"
   },
   "source": [
    "# Binarized state spaces\n",
    "\n",
    "Use agent to train efficiently on `CartPole-v0`. This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
    "\n",
    "The simplest way is to use `round(x, n_digits)` (or `np.round`) to round a real number to a given amount of digits. The tricky part is to get the `n_digits` right for each state to train effectively.\n",
    "\n",
    "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1632664931899,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "TpGOX4s7xWUl",
    "outputId": "5869153e-d0a6-4050-cadf-357f6b6e1a25"
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return gym.make('CartPole-v0').env  # .env unwraps the TimeLimit wrapper\n",
    "\n",
    "env = make_env()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state: %s\" % (env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dkd97DeUxWUl"
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1632664934252,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "E_6EE95kxWUm"
   },
   "outputs": [],
   "source": [
    "def visualize_cartpole_observation_distribution(seen_observations):\n",
    "    seen_observations = np.array(seen_observations)\n",
    "    \n",
    "    # The meaning of the observations is documented in\n",
    "    # https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "    f, axarr = plt.subplots(2, 2, figsize=(16, 9), sharey=True)\n",
    "    for i, title in enumerate(['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity At Tip']):\n",
    "        ax = axarr[i // 2, i % 2]\n",
    "        ax.hist(seen_observations[:, i], bins=20)\n",
    "        ax.set_title(title)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n",
    "        ax.grid()\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "executionInfo": {
     "elapsed": 1665,
     "status": "ok",
     "timestamp": 1632664937654,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "ZQoU3QCGxWUm",
    "outputId": "0e3d435a-446b-4980-e6ea-d1aead53db13"
   },
   "outputs": [],
   "source": [
    "seen_observations = []\n",
    "for _ in range(1000):\n",
    "    seen_observations.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "        seen_observations.append(s)\n",
    "\n",
    "visualize_cartpole_observation_distribution(seen_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kjwLlsqxWUm"
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1632664939277,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "WZyCLLhmxWUn"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def observation(self, state):\n",
    "        # Hint: you can do that with round(x, n_digits).\n",
    "        # You may pick a different n_digits for each dimension.\n",
    "        state[0] = np.round(state[0], 0)\n",
    "        state[1] = np.round(state[1], 1)\n",
    "        state[2] = np.round(state[2], 2)\n",
    "        state[3] = np.round(state[3], 1)\n",
    "\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1632665206448,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "O6HLYtpfxWUn"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(make_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1632664944617,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "_LcfDwlJxWUn",
    "outputId": "1e4944bc-399e-4961-8014-d98c2fe3b5ee"
   },
   "outputs": [],
   "source": [
    "seen_observations = []\n",
    "for _ in range(1000):\n",
    "    seen_observations.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "        seen_observations.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "visualize_cartpole_observation_distribution(seen_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szISJ2uFxWUn"
   },
   "source": [
    "## Learn binarized policy\n",
    "\n",
    "Now let's train a policy that uses binarized state space.\n",
    "\n",
    "__Tips:__\n",
    "\n",
    "* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n",
    "* If your binarization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce epsilon decay or change binarization. In practice we found that this kind of mistake is rather frequent.\n",
    "* If your binarization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n",
    "* **Start with a coarse binarization** and make it more fine-grained if that seems necessary.\n",
    "* Having $10^3$–$10^4$ distinct states is recommended (`len(agent._qvalues)`), but not required.\n",
    "* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n",
    "\n",
    "A reasonable agent should attain an average reward of at least 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1632665202029,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "pFfMTVMHxWUo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1632665203014,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "WeGXvbYixWUo"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 300292,
     "status": "ok",
     "timestamp": 1632665509411,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "Rh0P9JQxxWUo",
    "outputId": "9b2b1101-3e1e-4d0f-bea6-7aede3eb0ef4"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "epsilons = []\n",
    "\n",
    "for i in range(10000):\n",
    "    reward = play_and_train(env, agent)\n",
    "    rewards.append(reward)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    \n",
    "    agent.epsilon *= 0.9999\n",
    "    if i % 100 == 0:\n",
    "        rewards_ewma = moving_average(rewards)\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.plot(rewards, label='rewards')\n",
    "        plt.plot(rewards_ewma, label='rewards ewma@100')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.title('eps = {:e}, rewards ewma@100 = {:.1f}'.format(agent.epsilon, rewards_ewma[-1]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7LNykG1xWUp"
   },
   "source": [
    "### Submit to Coursera II: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1632665545692,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "QAGlV0e1xWUx"
   },
   "outputs": [],
   "source": [
    "submit_rewards2 = rewards.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1632665553746,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "KNZWQZd0xWUx",
    "outputId": "755feb05-7a8c-4273-ea81-3dca5a4b6b96"
   },
   "outputs": [],
   "source": [
    "from submit import submit_qlearning\n",
    "submit_qlearning(submit_rewards1, submit_rewards2, 'kmfrick98@gmail.com', '')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de qlearning.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/yandexdataschool/Practical_RL/blob/coursera/week3_model_free/qlearning.ipynb",
     "timestamp": 1632654190661
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
