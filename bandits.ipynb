{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1634401542128,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "cAwVfsoS4DBJ"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/submit.py\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1634401542534,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "mYx6LkBQ4DBP"
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import enum\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrVoPJz24DBP"
   },
   "source": [
    "## Bernoulli Bandit\n",
    "\n",
    "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
    "\n",
    "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
    "\n",
    "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$\n",
    "\n",
    "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
    "\n",
    "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1634401542535,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "_YBcaFCi4DBT"
   },
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    def __init__(self, n_actions=5):\n",
    "        self._probs = np.random.random(n_actions)\n",
    "\n",
    "    @property\n",
    "    def action_count(self):\n",
    "        return len(self._probs)\n",
    "\n",
    "    def pull(self, action):\n",
    "        if np.any(np.random.random() > self._probs[action]):\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "\n",
    "    def optimal_reward(self):\n",
    "        \"\"\" Used for regret calculation\n",
    "        \"\"\"\n",
    "        return np.max(self._probs)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" Used in nonstationary version\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Used in nonstationary version\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1634401542536,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "RAkZywu04DBU"
   },
   "outputs": [],
   "source": [
    "class AbstractAgent(metaclass=ABCMeta):\n",
    "    def init_actions(self, n_actions):\n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._total_pulls = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get current best action\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Observe reward from action and update agent's internal parameters\n",
    "        :type action: int\n",
    "        :type reward: int\n",
    "        \"\"\"\n",
    "        self._total_pulls += 1\n",
    "        if reward == 1:\n",
    "            self._successes[action] += 1\n",
    "        else:\n",
    "            self._failures[action] += 1\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class RandomAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        return np.random.randint(0, len(self._successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suI3Qsq04DBU"
   },
   "source": [
    "### Epsilon-greedy agent\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "Implement the algorithm above in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1634401542536,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "d3gPtEw54DBV"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    def __init__(self, epsilon=0.01):\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "    def get_action(self):\n",
    "        p = np.random.uniform(0, 1)\n",
    "        if p < self._epsilon:\n",
    "          return np.random.choice(np.arange(len(self._successes)))\n",
    "        else:\n",
    "          idx = []\n",
    "          qvals = []\n",
    "          for a in np.arange(len(self._successes)):\n",
    "            qvals.append(self._successes[a]/(self._successes[a]+self._failures[a]))\n",
    "            idx.append(a)\n",
    "\n",
    "        return idx[np.argmax(qvals)]\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v38A1k7c4DBV"
   },
   "source": [
    "### UCB Agent\n",
    "Epsilon-greedy strategy heve no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents otimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2log\\ t \\ / \\ (\\alpha_k + \\beta_k)}$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    " $x_t \\leftarrow argmax_{k}w$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "__Note:__ in practice, one can multiply $\\sqrt{2log\\ t \\ / \\ (\\alpha_k + \\beta_k)}$ by some tunable parameter to regulate agent's optimism and wilingness to abandon non-promising actions.\n",
    "\n",
    "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1634401553130,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "UazglDpX4DBW"
   },
   "outputs": [],
   "source": [
    "class UCBAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "      qvals = []\n",
    "      for a in np.arange(len(self._successes)):\n",
    "        qvals.append(self._successes[a]/(self._successes[a]+self._failures[a]) + np.sqrt(2*np.log10(self._total_pulls)/(self._successes[a]+self._failures[a])))\n",
    "      return np.argmax(qvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVCBkvjm4DBW"
   },
   "source": [
    "### Thompson sampling\n",
    "\n",
    "UCB1 algorithm does not take into account actual distribution of rewards. If we know the distribution - we can do much better by using Thompson sampling:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample $\\hat\\theta_k \\sim beta(\\alpha_k, \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    " \n",
    "\n",
    "More on Thompson Sampling:\n",
    "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634401542538,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "fjBaWroX4DBX"
   },
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        qvals = np.random.beta(self._successes+1, self._failures+1)\n",
    "        return np.argmax(qvals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634401542538,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "oJsWkHwr4DBX"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_regret(env, agents, n_steps=5000, n_trials=50):\n",
    "    scores = OrderedDict({\n",
    "        agent.name: [0.0 for step in range(n_steps)] for agent in agents\n",
    "    })\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        env.reset()\n",
    "\n",
    "        for a in agents:\n",
    "            a.init_actions(env.action_count)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            optimal_reward = env.optimal_reward()\n",
    "\n",
    "            for agent in agents:\n",
    "                action = agent.get_action()\n",
    "                reward = env.pull(action)\n",
    "                agent.update(action, reward)\n",
    "                scores[agent.name][i] += optimal_reward - reward\n",
    "\n",
    "            env.step()  # change bandit's state if it is unstationary\n",
    "\n",
    "    for agent in agents:\n",
    "        scores[agent.name] = np.cumsum(scores[agent.name]) / n_trials\n",
    "\n",
    "    return scores\n",
    "\n",
    "def plot_regret(agents, scores):\n",
    "    for agent in agents:\n",
    "        plt.plot(scores[agent.name])\n",
    "\n",
    "    plt.legend([agent.name for agent in agents])\n",
    "\n",
    "    plt.ylabel(\"regret\")\n",
    "    plt.xlabel(\"steps\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 15779,
     "status": "ok",
     "timestamp": 1634401571160,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "3ZywF7Zr4DBY",
    "outputId": "c1caeb72-7089-4f76-eee2-0c200ed240a6"
   },
   "outputs": [],
   "source": [
    "# Uncomment agents\n",
    "agents = [\n",
    "         EpsilonGreedyAgent(),\n",
    "         UCBAgent(),\n",
    "         ThompsonSamplingAgent()\n",
    "]\n",
    "\n",
    "regret = get_regret(BernoulliBandit(), agents, n_steps=10000, n_trials=10)\n",
    "plot_regret(agents, regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZC7k1vG4DBa"
   },
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1634401603645,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "qeU9_p3n4DBb",
    "outputId": "5a523901-f3ff-44d7-f87b-b04a6b589a59"
   },
   "outputs": [],
   "source": [
    "from submit import submit_bandits\n",
    "\n",
    "submit_bandits(agents, regret, 'kmfrick98@gmail.com', '')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copia de bandits.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/yandexdataschool/Practical_RL/blob/coursera/week6_outro/bandits.ipynb",
     "timestamp": 1634400746766
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
