{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYOO6-UDsAnp"
   },
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/blob/36a0b58261acde756abd55306fbe63df226bf62b/hw2/HW2.ipynb) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgb8cdwzsAnt"
   },
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30490,
     "status": "ok",
     "timestamp": 1632652877699,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "AETn8BWfsAnu",
    "outputId": "be874672-cd44-40dc-dace-97aa23d1fd33"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week2_model_based/submit.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week2_model_based/mdp.py\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1449,
     "status": "ok",
     "timestamp": 1632652924799,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "GICAy7DrsAnw"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUeIDIOssAnx"
   },
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1632652927797,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "Z1MgdAIdsAny",
    "outputId": "0f065885-11e5-4041-dbe0-ae05d5dc7d6e"
   },
   "outputs": [],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x9ERy8lsAn1"
   },
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1632652958634,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "vIlDGrPLsAn2",
    "outputId": "91938cef-e1b5-4e74-e953-49a25886e580"
   },
   "outputs": [],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk6jzGZmsAn2"
   },
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. \n",
    "\n",
    "1. * For ubuntu just run: `sudo apt-get install graphviz` \n",
    "   * For OSX: `brew install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA3O2vfBsAn5",
    "outputId": "85c88907-35f8-4cd3-e8e9-2bc7e70fd472"
   },
   "outputs": [],
   "source": [
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCW8wQj9sAn6",
    "outputId": "ebcdecd8-bc3f-45e3-cd3d-3aaf75d35051"
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUqh8DSksAn7"
   },
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEMfotHCsAn7"
   },
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1632653406332,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "E_mjH1n6sAn8"
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    q = 0\n",
    "    for s in mdp.get_all_states():\n",
    "      q += mdp.get_transition_prob(state, action, s) * (mdp.get_reward(state, action, s) + gamma * state_values[s])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1632653401678,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "fDbSLCSNsAn8",
    "outputId": "865cb7a0-e4c9-4897-fe90-ff7bf02a399f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOXed5kEsAn8"
   },
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1632653591275,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "OzxakXrHsAn9"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    q = []\n",
    "    for a in mdp.get_possible_actions(state):\n",
    "      q.append(get_action_value(mdp, state_values, state, a, gamma))\n",
    "    return max(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632653592747,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "0YXZvEsMsAn9"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
    "assert test_Vs == test_Vs_copy, \"Please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_okiURb8sAn-"
   },
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1632653739425,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "GpEwxy8ZsAn_",
    "outputId": "95d7aacd-4f8f-4af1-ea32-93cf9a689973"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "has_graphviz = False\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = dict()\n",
    "    for s in mdp.get_all_states():\n",
    "      new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeQWRNE2sAn_"
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1632653745604,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "aB_roDUmsAoA",
    "outputId": "dd261dd5-cb07-497c-cbef-67d02865f59a"
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7fbyQzbsAoA"
   },
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632653925457,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "xM3wyrUzsAoA"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    q = dict()\n",
    "    for a in mdp.get_possible_actions(state):\n",
    "      q[a] = get_action_value(mdp, state_values, state, a, gamma)\n",
    "    return max(q, key=lambda key : q[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632653928195,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "LFlPvozUsAoA"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a1'\n",
    "\n",
    "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
    "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OixnuIQVsAoB"
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1632653933257,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "_WaD_jB1sAoB",
    "outputId": "664bfae5-9c8e-4293-d6e9-5160f9790ee7"
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4AKThlhsAoB"
   },
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1632653937743,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "pMo9TUftsAoB",
    "outputId": "a37e928c-e2c7-4e89-c5fb-8432d306bf28"
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1632654006774,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "rnm7AL_nsAoC"
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = dict()\n",
    "        for s in mdp.get_all_states():\n",
    "          new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s])\n",
    "                   for s in mdp.get_all_states())\n",
    "\n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
    "              (i, diff, new_state_values[mdp._initial_state]))\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1632654008159,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "C9bQyGVusAoC",
    "outputId": "bdbf47c7-bb4e-4cee-b0dc-9c3c5790d0ad"
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1632654011236,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "2s2p_A17sAoD",
    "outputId": "631bae62-ef68-48fb-90cf-4ec8baecb4b9"
   },
   "outputs": [],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjXtyECHsAoD"
   },
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaqlreRLsAoE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (0, 1)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
    "                      head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa1MLUkKsAoE"
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL4kne1ksAoE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmQuiSX6sAoF"
   },
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1517,
     "status": "ok",
     "timestamp": 1632654021734,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "qPBJfnB7sAoF",
    "outputId": "e142544c-62bd-4944-ba8d-a92bba2cfabb"
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2281,
     "status": "ok",
     "timestamp": 1632654026761,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "MGtbUcGLsAoF",
    "outputId": "7e723782-83e3-40d1-da01-be2f2432b94c"
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1936,
     "status": "ok",
     "timestamp": 1632654030205,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "vMcDieVWsAoG",
    "outputId": "6b63773a-7694-497e-d6c2-0e97010ac1be"
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13742,
     "status": "ok",
     "timestamp": 1632654046605,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "z-3d7wRTsAoG",
    "outputId": "ecd0a19a-3db5-4417-9b18-a6d4cc9bffcc"
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYVxsKt9sAoG"
   },
   "source": [
    "### Submit to coursera\n",
    "\n",
    "If your submission doesn't finish in 30 seconds, set `verbose=True` and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16773,
     "status": "ok",
     "timestamp": 1632654086650,
     "user": {
      "displayName": "Kevin Michael Frick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwSv97gy1jgHAEw4EI6t5LVDOVXV-_FtdCM8n5xg=s64",
      "userId": "12741784334941682893"
     },
     "user_tz": -120
    },
    "id": "M-4KtKEusAoH",
    "outputId": "ab39ba23-2845-477d-ff9e-dc973c5e9fc8"
   },
   "outputs": [],
   "source": [
    "from submit import submit_assigment\n",
    "submit_assigment(\n",
    "    get_action_value,\n",
    "    get_new_state_value,\n",
    "    get_optimal_action,\n",
    "    value_iteration,\n",
    "    'kmfrick98@gmail.com',\n",
    "    'lPJxdj9USdO9sHUe',\n",
    "    verbose=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de practice_vi.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/yandexdataschool/Practical_RL/blob/coursera/week2_model_based/practice_vi.ipynb",
     "timestamp": 1632652820465
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
